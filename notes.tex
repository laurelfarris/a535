\documentclass[12pt]{article}
\usepackage[margin=1in]{geometry}
\usepackage{color}
\usepackage{xcolor}
\usepackage{hyperref}
\usepackage{amsmath}

%\usepackage[usenames, dvipsnames]{color}
% \definecolor{mypink1}{rgb}{0.858, 0.188, 0.478}
% \definecolor{mypink2}{RGB}{219, 48, 122}
% \definecolor{mypink3}{cmyk}{0, 0.7808, 0.4429, 0.1412}
% \definecolor{mygray}{gray}{0.6}

\definecolor{myBlue}{rgb}{0.2,0.2,0.6}

%  \begin{document}
%  User-defined colours with different colour models:
%
%   \begin{enumerate}
%   \item \textcolor{mypink1}{Pink with rgb}
%   \item \textcolor{mypink2}{Pink with RGB}
%   \item \textcolor{mypink3}{Pink with cmyk}
%   \item \textcolor{mygray}{Gray with gray}
%   \end{enumerate}

\setlength{\parindent}{0em}
\setlength{\parskip}{0.5em}

\title{\vspace{-0.75in}ASTR 535 Lecture Notes}
\author{Jon Holtzman}
\date{Spring 2016}

\begin{document}
\maketitle

course website: \textcolor{blue}
{\url{http://astronomy.nmsu.edu/holtz/a535}}

\textcolor{magenta}{Friday, January 22}
\section*{Properties of light, magnitudes, errors, and error analysis}

\subsection*{Light}
Wavelength regimes:
\begin{itemize}
    \item gamma rays
    \item x-rays
    \item ultraviolent (UV)
        \begin{itemize}
            \item near: 900--3500 \AA{}
            \item far: 100--900 \AA{}
        \end{itemize}
        The 900 \AA{} break is because of the Lyman limit at 912 \AA{}.
        This is where neutral hydrogen is ionized, so the universe is largely
        opaque to wavelengths shorter than this.
    \item visual (V): 4000--7000 \AA{}
    (note that `V' is different from `optical',
        which is slightly broader: 3500--10000 \AA{}. The 3500 \AA{} cutoff
        is due to the Earth's atmosphere being opaque to wavelengths shorter
        than this).
    \item IR
        \begin{itemize}
            \item near: 1--5 $\mu$ (1--10 $\mu$ in online notes)
            \item mid: (10--100 $\mu$)
            \item far: 5--100 $\mu$ (100--1000 $\mu$)
        \end{itemize}
    \item sub-mm 500--1000 $\mu$
    \item microwave
    \item radio
\end{itemize}
Quantities of light:
\begin{itemize}
    \item Intensity $I(\theta,\phi)$ [erg s$^{-1}\ \nu^{-1}\ \Omega^{-1}$]:
        Encapsulates $direction$ light is coming from.
        Also known as radiance.
    \item Surface Brightness (SB)
        [erg s$^{-1}$ cm$^{-2}$ $\nu^{-1}$ sterradian$^{-1}$]:
        amount of energy $received$ in a unit surface
        element per unit time per unit frequency (or wavelength)
        from a unit
        solid angle in the direction ($\theta,\phi$), where $\theta$
        is the angle
        away from the normal to the surface element, and $\phi$ is the
        azimuthal angle.
        To calculate SB, just divide the flux by the angle subtended
        by the object [rad$^2$]. At a larger distance, the flux will
        be smaller, but so will the angle subtended by the object, so
        SB is independent of distance unless considering cosmological
        scales, where the curvature of spacetime has an effect.
    \item Flux (F) [erg s$^{-1}$ cm$^{-2}\ \nu^{-1}$]:
        amount of energy passing through a unit surface element
        in all directions, defined by
        \begin{equation}
            F_{\nu} = \int I_{\nu}\cos(\theta)\textrm{d}\Omega
        \end{equation}
        where d$\Omega$ is the solid angle element, and the integration is
        over the entire solid angle. The $\cos(\theta)$ factor is important
        for, e.g., ISM where light is coming from all directions, but for
        tiny objects, $\theta$ is negligibally small and can be dropped.
        Integrates over $all$ directions.
        Also known as irradiance.
    \item Luminosity (L) [erg s$^{-1}$]:
        $intrinsic$ energy emitted by the source per
        second ($\sim$ power). For an isotropically emitting source,
        \begin{equation}
            L = 4 \pi d^2 F
        \end{equation}
        where $d$ = distance to source (so L can only be calculated if
        the distance is known). Also known as radiant flux.
\end{itemize}
What to measure for sources:
\begin{itemize}
    \item Resolved: directly measure surface brightness (intensity)
        distribution on the sky, usually over some bandpass or wavelength
        interval.
    \item Unresolved: measure the flux. Diffraction is the reason stellar
        surfaces cannot be resolved. Because of this, we cannot measure
        SB, so we measure flux, integrated over the entire object.
\end{itemize}

Questions:
\begin{itemize}
    \item What are the dimensions of the three quantities: luminosity,
        surface brightness (intensity), and flux?
    \item How do the three quantities depend on distance to the source?
    \item To what quantity is apparent magnitude of a star related?
    \item To what quantity is the absolute magnitude related?
\end{itemize}
Amount of light emitted is a function of wavelength,
so we are often interested
in e.g.\ flux per unit wavelength (or frequency),
also known as $specific$ flux.
Using $\lambda=\frac{c}{\nu} \rightarrow
\frac{\textrm{d}\lambda}{\textrm{d}\nu} = \frac{-c}{\nu^2}$
\begin{align*}
    \int F_{\nu} \textrm{d} \nu &= -\int F_{\lambda} \textrm{d} \lambda\\
    F_{\nu} \textrm{d} \nu &= -F_{\lambda} \textrm{d} \lambda\\
    F_{\nu} &= -F_{\lambda} \frac{\textrm{d} \lambda}{\textrm{d}\nu}\\
    &= F_{\lambda} \frac{c}{\nu^2}\\
    &= F_{\lambda} \frac{\lambda^2}{c}\\
\end{align*}
The negative comes from frequency and wavelength increasing in
opposite directions.
Note that a constant $F_{\lambda}$ implies a $non$-constant $F_{\nu}$
and vice versa. Depending on where you are, a constant chunk of 1 Hz is
not the same wavelength range.

Units: often cgs, magnitudes, Jansky (a flux density unit
corresponding to 10$^{-26}$ W m$^{-2}$ Hz$^{-1}$)

There are often variations in terminology

Terminology of measurements:
\begin{itemize}
    \item photometry (broad-band flux measurement): SB or flux, integrated
        over some wavelength range.
    \item spectroscopy (\emph{relative} measurement of fluxes at
        different wavelengths):
        $f(\lambda)$
    \item spectrophotometry (\emph{absolute} measurement of fluxes at
        different wavelengths):
        $f(\lambda)$
    \item astrometry: concerned with positions of observed flux, not brightness,
        but direction.
    \item morphology: intensity as a function of position;
        often, absolute measurements are unimportant. Deals with $resolved$
        objects, intensity as function of position.
\end{itemize}
Generally, measure \emph{flux} with photometry, and flux
\emph{density} (per unit wavelength)  with spectroscopy
(down to the resolution of the spectrograph).
In practice, with most detectors,
we measure photon flux [photons cm$^{-2}$ s$^{-1}$]
with a photon counting device,
rather than energy flux (which is done with bolometers).
The monochromatic photon flux is given by the
energy flux ($F_{\lambda}$)
divided by the
energy per photon ($E_{photon} = \frac{hc}{\lambda}$), or
$$ \textrm{photon\ flux} = \int F_{\lambda}
    \frac{\lambda}{hc} \textrm{d} \lambda $$

% Magnitudes and photometric systems

\subsection*{Magnitudes and photometric systems}
Magnitudes are related to flux (and SB and L) by:
    $$    m_1 - m_2 = -2.5 \log \frac{b_1}{b_2} $$
or for a single object:
\begin{align*}
    m &= -2.5 \log \frac{F}{F_0}\\
      &= -2.5 \log F + 2.5 \log F_0
\end{align*}
where the coefficient of proportionality, $F_0$, depends on the definition
of photometric system; the quantity $2.5 \log F_0$ may be referred to as
the photometric system zeropoint. Note that this relationship holds
regardless of what photometric system you are using. Inverting, one gets:
    $$ F = F_0 \times 10^{-0.4\textrm{m}} $$
Just as fluxes can be represented in magnitude units, flux \emph{densities}
(flux per unit wavelength/frequency, or \emph{monochromatic} flux,
as opposed to integrated flux)
can be specified by monochromatic magnitudes:
\begin{equation*}
    F_{\lambda} = F_0 (\lambda) \times 10^{-0.4 \textrm{m}(\lambda)}
\end{equation*}
although spectra are more often given in flux units than in magnitude units.
Note that it is possible that $F_0$ is a function of wavelength.

Since magnitudes are logarithmic, the $difference$ between
magnitudes corresponds to a ratio of fluxes; ratios of magnitudes are
generally unphysical. If one is just doing relative measurements of
brightness between objects, this can be done without knowledge of $F_0$
(or, equivalently, the system zeropoint); objects that differ in brightness
by $\Delta$M have the same ratio of brightness (10$^{-0.4 \Delta M}$)
regardless of what photometric system they are in.
\textcolor{red}{The photometric system
definitions and zeropoints are only needed when converting between calibrated
magnitudes and fluxes}. Note that this means that if one references the
brightness of one object relative to that of another, a magnitude system
can be set up relative to the brightness of the reference source. However, the
utility of a system when doing astrophysics generally requires an
understanding of the actual fluxes.

\textcolor{magenta}{Monday, January 25}

There are three main types of magnitude systems in use in astronomy:
STMAG, ABNU, and VEGAMAG.
We start by describing the two simpler ones: the STMAG and the ABNU mag system.
In these simple systems, the reference flux is just a constant value in
$F_{\lambda}$ or $F_{\nu}$. However, these are not always the most widely used
systems in astronomy, because
\textcolor{red}{no natural source exists with a flat spectrum}.

The STMAG (Space Telescope MAGnitude) is defined
relative to a source of constant $F_{\lambda}$. In this system,
the reference flux is given by
\begin{equation*}
    F_{0,\lambda} = 3.60 \times 10^{-9}\ \textrm{erg\ s}^{-1}\
    \textrm{cm}^{-2}\ \textrm{\AA{}}^{-1}
\end{equation*}
which is equal to the flux of Vega at 5500\AA{};
hence a star of Vega's brightness at 5500\AA{} is defined to have m=0.
Alternatively (using $m = -2.5\log \frac{F}{F_0}$), we can write
\begin{equation*}
    m_{\textrm{STMAG}} = -2.5 \log F_{\lambda} - 21.1
\end{equation*}
for $F_{\lambda}$ in \textcolor{red}{cgs units}(using cm$^{-2}$ \AA{}$^{-1}$
distinguishes between the collecting area and the wavelength, since both are
units of distance. Be careful with units when doing these conversions; as
long as the proper flux units come out at the end, the answer should be
correct).

The ABNU system is defined relative to a source
of constant $F_{\nu}$ and we have
\begin{equation*}
    F_{0,\nu} = 3.63 \times 10^{-20}\ \textrm{erg\ s}^{-1}\
    \textrm{cm}^{-2}\ \textrm{Hz}^{-1}
\end{equation*}
or
\begin{equation*}
    m_{\textrm{ABNU}} = -2.5 \log F_{\nu} - 48.6
\end{equation*}
for $F_{\nu}$ in \textcolor{red}{cgs units}.
Again, the constant comes from the flux of Vega.

Magnitudes usually refer to the \emph{integrated} flux (over a
spectral bandpass, not just a single wavelength).
In this case, $F$ and $F_0$ refer to such integrated fluxes.
The STMAG and ABNU integrated systems are defined relative to sources
of constant $F_{\lambda}$ and $F_{\nu}$, respectively
\begin{align*}
    m_{\textrm{STMAG}} &= -2.5 \log \frac{\int F_{\lambda} \lambda
    \textrm{d}\lambda}{\int3.6\times10^{-9}\lambda\textrm{d}\lambda}\\
    m_{\textrm{ABNU}} &= -2.5 \log \frac{\int F_{\nu}\nu^{-1}
\textrm{d}\nu}{\int 3.6 \times 10^{-20}\nu^{-1}\textrm{d}\nu}
\end{align*}
These are defined to be the same at 5500\AA{}.
The factors of $\lambda$ and $\nu$ come from the conversion factor
$hc/\lambda$ for photon-counting detectors, where $h$ and $c$ cancel out in the
fractions in each equation.

Note that these systems differ by more than a constant,
because one is defined by units of $F_{\lambda}$ and the other by $F_{\nu}$,
so the difference betwen the systems is a function of wavelength.
(Question: what's the relations between $m_{STMAG}$ and $m_{ABNU}$?)\\

\noindent Note also that, using magnitudes,
\textcolor{red}{the measured magnitude is
nearly independent of bandpass width}, so a broader bandpass does not
imply a brighter (smaller) magnitude, which is not the case for
fluxes. The reference is being integrated as well, so they cancel.\\

\noindent The standard UBVRI broadband photometric system, as well as
several other magnitude systems, however, are not defined for a
constant (flat) $F_{\lambda}$ or $F_{\nu}$ spectrum; rather, they are defined
relative to the spectrum of an A0V star. Most systems are defined (or
at least were originally) to have the magnitude of Vega be zero in all
bandpasses (VEGAMAGS); if you ever get into this in detail, note that
this is not exactly true for the UBVRI system.\\

\noindent For the broadband UBVRI system, we have
\begin{equation*}
    m_{\textrm{UBVRI}} \approx -2.5 \log
    \frac{\int_{UBVRI}F_{\lambda}(object)\lambda\textrm{d}\lambda}
    {\int_{UBVRI}F_{\lambda}(Vega)\lambda\textrm{d}\lambda}
\end{equation*}
(as above, the factor of $\lambda$ comes in for photon counting
detectors). This gives the magnitude in U, B, V, R, \emph{or} I,
by integrating over that same bandpass.
The UBVRI filter set had overlapping bandpasses, so
there was a switch to interference filter: the ugriz system used
by SDSS (explained below... I think).\\

\noindent Here is a
\href{http://astronomy.nmsu.edu/holtz/a535/html/diagrams/a535/mag.htm}
{\textcolor{blue}{plot}}
to demonstrate the difference between the different systems.\\

\noindent While it seems that STMAG and ABNU systems are more
straightforward, in practice it is difficult to measure absolute
fluxes, and much easier to measure relative fluxes between objects.
Hence, historically observations were tied to observations of Vega (or
to stars which themselves were tied to Vega), so VEGAMAGs made sense,
and the issue of determining physical fluxes boiled down to measuring
the physical flux of Vega. Today, in some cases, it may be more
accurate to measure the absolute throughput of an instrumental system,
and using STMAG or ABNU makes more sense.

\subsubsection*{Colors}
Working in magnitudes, the difference in magnitudes between different
bandpasses (called the color index, or simply, color) is related to
the flux ratio between the bandpasses, i.e., the color.
In the UBVRI
system, the \emph{difference between magnitudes}, e.g. B-V,
gives the ratio of the fluxes in different bandpasses
\emph{relative to the ratio of the fluxes of
an A0V star in the different bandpasses (for VEGAMAG)}.
Note the typical colors of astronomical objects,
which are different for the different photometric systems.
Type `A' stars have color 0, and have the same SED as Vega.
Type `O' stars have color less than 0,
while cooler stars have color greater than 0.
In sloan system, have  g-r (ugriz). g-r=0 indicates constant
$F_{\nu}$.
\begin{equation*}
    m = -2.5\log\frac{\int_B F_{\lambda}\textrm{d}\lambda}
    {\int_V F_{\lambda}\textrm{d}\lambda}
\end{equation*}
if B-V=0, then (B/V)$_{object}$ is the same as (B/V)$_{Vega}$,
or $\Big(\frac{\int F_{\nu}}{\int F_{\lambda}}\Big)_{obj} =
    \Big(\frac{\int F_{\nu}}{\int F_{\lambda}}\Big)_{Vega} $.


What would the flux be from an object with some magnitude,
x? Need this to know how much observing time I need. E.g., convert the
spectrum of an elliptical galaxy to color; if you know F in one
filter, you can get F in another filter.

Which is closer to the UBVRI system, STMAG or ABNU?

What would typical colors be in a STMAG or ABNU system?

\textcolor{magenta}{Wednesday, January 27}

\subsubsection*{UBVRI magnitudes-flux conversion
}
To \textcolor{red}{convert Vega-based magnitudes to fluxes},
look up the flux of Vega at the center of the passband;
however, if the spectrum of the object
differs from that of Vega, this won't be perfectly accurate.
Given UBVRI magnitudes of an object in the desired band, filter profiles
(e.g. Bessell 1990, PASP 102,1181), and absolute spectrophotometry of
Vega (e.g., \href{http://adsabs.harvard.edu/abs/2004AJ....127.3508B}
{\textcolor{blue}{Bohlin \& Gilliland 2004, AJ 127, 3508}},
one can determine the flux.

If one wanted to estimate the flux of some object in
arbitrary bandpass given just the V magnitude of an object (a common
situation used when trying to predict exposures times, see below),
this can be done if an estimate of the spectral energy distribution
(SED) can be made; given the filter profiles, one can compute the
integral of the SED over the V bandpass, determine the scaling by
comparing with the integral of the Vega spectrum over the same
bandpass, then use the normalized SED to compute the flux in any
desired bandpass. Some possibly useful references for SEDs are:
Bruzual, Persson, Gunn, \& Stryker; Hunter, Christian, \& Jacoby;
Kurucz).

Things are certainly simpler in the ABNU or STMAG system, and
there has been some movement in this direction: the STScI gives STMAG
calibrations for HST instruments, and the SDSS photometric system is
close to an ABNU system.

Note, however, that even when the systems are conceptually
well defined, determining the absolute calibration of any photometric
system is very difficult in reality, and determining absolute fluxes
to the 1\% level is very challenging.

As a separate note on magnitudes themselves, note that some
people, in particular, the SDSS, have adopted a modified type of
magnitudes, called asinh magnitudes, which behave like normal (also
known as Pogson) magnitude for brighter objects, but have different
behavior for very faint objects (near the detection threshold); see
\href{http://adsabs.harvard.edu/abs/1999AJ....118.1406L}
{\textcolor{blue}{Lupton, Gunn, \& Szalay 1999 AJ 118, 1406}}
for details.

%---------------------------------------------------------------%

\subsection*{Observed fluxes and the count equation}
What if you are measuring flux with an actual instrument, i.e.\
counting photons? The intrinsic photon flux from the source is not
trivial to determine from the number of photons that you count. To get
the number of photons that you count in an observation, you need to
take into account
\begin{itemize}
    \item The area of your photon collector (telescope)
    \item Photon losses and gains from the Earth's atmosphere
    (which change with conditions)
    \item The efficiency of your collection/detection
    apparatus (which can change with time).
\end{itemize}
Generally, the astronomical signal (which might be a flux or a
surface brightness, depending on whether the object is resolved)
can be written in the form of the \emph{\textbf{count equation:}}
    $$ S = Tt \int \frac{F_{\lambda}}{\frac{hc}{\lambda}}q_{\lambda}
    a_{\lambda}\textrm{d}\lambda \equiv TtS' $$
    \begin{itemize}
        \item $S$: total number of photons observed (the ``signal")
        \item $S'$ is an observed flux rate,
        i.e. with all of the real details of the observing system included.
        \item $a_{\lambda}$: atmospheric transmission,
        or \emph{throughput}, which is the fraction of photons that
        make it through.
        (a typical value is about 0.9; $\sim$90\% of photons)
        \item $q_{\lambda}$ is the system efficiency
        (which includes telescope efficiency, instrument efficiency,
        filters, and detector)
        \item $T$: telescope collecting area
        \item $t$ is the integration time
        (total amount of time spent collecting photons).
        \item $F_{\lambda}/\frac{hc}{\lambda}$:
        flux divided by the energy per photon;
        gives the number of photons per second per square cm.
    \end{itemize}
$T$ and $t$ are the \emph{only} terms that do not depend on
the wavelength (or frequency).

Usually, however, one doesn't use this information to go
backward from $S$ to $F_{\lambda}$ because it is very
difficult to measure all of the terms precisely, and some of them
(e.g. $a$, and perhaps $q$) are time-variable; $a$ is also spatially
variable. Instead, most observations are performed differentially to a
set of other stars of well known brightness. If the stars of known
brightness are observed in the same observation, then the atmospheric
term is (approximately) the same for all stars; this is known as
\emph{differential photometry}.
From the photon flux of the object with known
brightness, one could determine an ``exposure efficiency''
or an ``effective area'' for this
exposure. Equivalently, and more commonly, one can calculate an
\emph{instrumental magnitude}:
    $$  m = -2.5 \log \frac{S}{t} $$
(i.e., normalize by the exposure time, $t$, to get counts/sec, although this
is not strictly necessary) and then determine the \emph{zeropoint},
$z$, that needs
to be added to give the calibrated magnitude, $M$
(which is still an \emph{apparent} magnitude).
    $$ M = m + z $$
Note that in the real world, one has to also consider possible
differences between a given experimental setup and the setup used to
measure the reference brightnesses, so this is only a first
approximation (i.e., the zeropoint may be different for different
stars with different spectral properties). If using instrumental mags
including exposure time normalization, the zeropoint gives the
magnitude of a star that will give 1 count/second.

\textcolor{myBlue}{FOV: usually in arcminute scales.
To go from observed to emitted $\rightarrow$ 2 different observations
(know flux of one: SDSS has list of stars with known brightness).
A star that is 10 times fainter than one with g=18 has g=20.5.
    $$ m = -2.5 \log(qF)$$
    $$ m = -2.5 \log F - 2.5\log q $$
the quantity `-2.5$\log q$' is the zeropoint.
    $$ m = -2.5 \log \frac{S}{t} $$
No nearby star... $q$ is still the same, but $a$ changes.
If sky is `well-behaved' atmospheric effects is simple function
of distance form zenith; slope of variation can change from one
night to the next.\\\\
How did standard stars get measured in the first place?
Someone had to do some hard work figuring out throughput.
Accuracy depends on number of photons, which will determine
how much observing time you will need.}

If there are no stars of known brightness in the same
observation, then calibration must be done against stars in other
observations. This then requires that the different effects of the
Earth's atmosphere in different locations in the sky be accounted for.
This is known as \emph{all-sky}, or absolute, photometry. To do this requires
that the sky is ``well-behaved", i.e. one can accurately predict the
atmospheric throughput as a function of position. This requires that
there be no clouds, i.e. \emph{photometric} weather. Differential photometry
can be done in non-photometric weather, hence it is much simpler. Of
course, it is always possible to obtain differential photometry and
then go back later and obtain absolute photometry of the reference
stars.

Of course, at some point, someone needs to figure out what
the fluxes of the calibrating stars really are, and this requires
understanding all of the terms in the count equation. It is
challenging, and often, absolute calibration of a system is uncertain
to a couple of percent.

It is also common to stop with differential photometry if
one is studying variable objects, i.e. where one is just interested in
the change in brightness of an object, not the absolute flux level. In
this case, one only has to reference the brightness of the target
object relative some other object (or ensemble of objects) in the
field that are non-variable.

While the count equation isn't usually used for calibration,
it is very commonly used for computing the approximate number of
photons you will receive from a given source in a given amount of time
for a given observational setup. This number is critical to know in
order to estimate your expected errors and exposure times in observing
proposals, observing runs, etc. Understanding errors in absolutely
critical in all sciences, and maybe even more so in astronomy, where
objects are faint, photons are scarce, and errors are not at all
insignificant. The count equation provides the basis for exposure time
calculator (ETC) programs, because it gives an expectation of the
number of photons that will be received by a given instrument as a
function of exposure time. As we will see shortly, this provides the
information we need to calculate the uncertainty in the measurement as
a function of exposure time.

%-----------------------------------------------------------------------%

\subsection*{Uncertainties in photon rates}
\textcolor{myBlue}{
Required accuracy - depends on the science you're doing.
Uncertainties (more descriptive than `errors') -
associated with flux measurements.
For given input rate, what is the probability of observing
range of all possible input values?
}

For a given rate of emitted photons, there's a probability function
which gives the number of photons we detect, even assuming 100\%
detection efficiency, because of \emph{statistical} uncertainties. In
addition, there may also be \emph{instrumental} uncertainties. Consequently,
we now turn to the concepts of probability distributions, with
particular interest in the distribution which applies to the detection
of photons.

\emph{Distributions and characteristics thereof}

concept of a distribution: define $p(x)dx$ as probability of
event occuring in $x + \textrm{d}x$:
        $$ \int p(x)\textrm{d}x = 1 $$
Some definitions relating to values which characterize a distribution:
\begin{align*}
    \textrm{mean} \equiv \mu &= \int xp(x)\textrm{d}x \\
    \textrm{variance} \equiv \sigma^2 &= \int (x-\mu)^2 p(x)\textrm{d}x \\
    \textrm{standard\ deviation} \equiv \sigma &= \sqrt{\textrm{variance}}
\end{align*}
median: mid-point value.
\begin{align*}
    \frac{ \int_{-\infty}^{x_{median}} p(x)\textrm{d}x }
    { \int_{-\infty}^{\infty} p(x)\textrm{d}x }
    &= \frac{1}{2}
\end{align*}
mode: most probable value (peak in plot).

\textcolor{magenta}{Monday, February 1}

Note that the geometric interpretation of above quantities
depends on the nature of the distribution; although we all carry
around the picture of the mean and the variance for a Gaussian
distribution, these pictures are not applicable to other
distributions, but the quantities are still well-defined.

Also, note that there is a difference between the
\emph{sample}
mean, variance, etc. and the \emph{population} quantities. The latter apply
to the true distribution, while the former are estimates of the latter
from some finite sample ($N$ measurements) of the population. The sample
quantities are derived from:
$$\textrm{sample\ mean:\ } \bar{x} \equiv \frac{\sum x_i}{N}$$
$$\textrm{sample\ variance:\ } \equiv
  \frac{\sum (x_i-\bar{x})^2}{N-1} =
  \frac{\sum x_i^2-(\sum x_i)^2/N}{N-1}$$
The sample mean and variance approach the true mean and variance as N
approaches infinity. But note, especially for small samples, your
estimate of the mean and variance may differ from their true
(population) values (more below).

\emph{The binomial distribution}

\textcolor{myBlue}{Given independent events (e.g. when rolling a set
of dice, the results of the second roll are independent of the first),
two parameters, $n$ and $p$, where $n$
is the total number of observations and $p$ is the probability of
observing $x$ during each observation. 
Observing photons:
For a binomial distribution $P(x,n,p)$, $x$ is the number of photons
received, $n$ is the number of
photons emitted by the source, and $p$ is the probability of detecting
\emph{each} photon. Integrating this actually simplifies it}

Now we consider what distribution is appropriate for the
detection of photons. The photon distribution can be derived from the
\emph{binomial} distribution, which gives the probability of observing the
number, $x$, of some possible event, given a total number of events $n$,
and the probability of observing the particular event (among all other
possibilities) in any single event, $p$, under the assumption that all
events are independent of each other:
   $$ P(x,n,p) = \frac{n!p^x(1-p)^{n-x}}{x!(n-x)!}  $$
For the binomial distribution, one can derive:
   $$ \textrm{mean} \equiv \mu \equiv \int xp(x)\textrm{d}x = np $$
   $$ \textrm{variance} \equiv \sigma^2 \equiv
      \int (x-\mu)^2p(x)\textrm{d}x = np(1-p) $$

\emph{The Poisson distribution}

\textcolor{myBlue}{Unlike a gaussian, a poisson distribution is not
symmetric about the mean (you can't detect a negative number of
photons). It cuts off at zero. THE VARIANCE IS EQUAL TO THE MEAN.
THEREFORE, THE STANDARD DEVIATION IS THE SQUARE ROOT OF THE MEAN.}
\textcolor{red}{This is the `key super important result'.}

In the case of detecting photons, $n$ is the total number of
photons emitted, and $p$ is the probability of detecting a photon during
our observation out of the total emitted. We don't know either of
these numbers. However, we do know that
\textcolor{red}{p$<<$1} and we know, or at
least we can estimate, the mean number detected:
  $$  \mu = np $$
In this limit, the binomial distribution asymtotically approaches the
\emph{Poisson} distribution:
   $$  P(x,\mu) = \frac{\mu^x e^{-\mu}}{x!} $$
From the expressions for the binomial distribution in this limit, the
mean of the distribution is $\mu$, and the variance is
  $$  \textrm{variance} = \sum_x [(x-\mu)^2p(x,\mu)] $$
  $$  \textrm{variance} = np = \mu  $$

\textcolor{red}{\textbf{This is an \emph{important result}}}.

Note that the Poisson distribution is generally the
appropriate distribution not only for counting photons, but for
\emph{any} sort of counting experiment where a series of events
occurs with a known average rate, and are independent of time
since the last event.

What does the Poisson distribution look like?
\href{http://astronomy.nmsu.edu/holtz/a535/html/diagrams/a535/poisson.htm}
{\textcolor{blue}{Plots}}
for $\mu$ = 2, $\mu$ = 10, $\mu$ = 10000.

\emph{The normal, or Gaussian, distribution}

\textcolor{myBlue}{factor into std. numerical technique (least
squares), photons not emitted at perfect constant rate, readout
function, applies to lots of stuff}.

Note, for large $\mu$, the Poisson distribution is
well-approximated around the peak by a \emph{Gaussian}, or
\emph{normal} distribution:
    $$ P(x,\mu,\sigma) = \frac{1}{\sigma\sqrt{2\pi}}
        e^{ -\frac{1}{2} (\frac{x-\mu}{\sigma})^2 }  $$
This is important because it allows us to use ``simple'' least squares
techniques to fit observational data, because these generally assume
normally distributed data. However, in the tails of the
distribution, and at low mean rates, the Poisson distribution can
differ significantly from a Gaussian distribution. In these cases,
least-squares may not be appropriate to model observational data;
instead, one might need to consider maximum likelihood techniques.

The normal distribution is also important because many
physical variables seem to be distributed accordingly. This may not be
an accident because of the \emph{central limit theorem}: if a quantity
depends on a number of independent random variables with ANY
distribution, the quantity itself will be distributed normally (see
statistics texts). In observations, we encounter the normal
distribution because \emph{readout} noise is distributed normally.
(Readout noise is one important source of \emph{instrumental noise}).

\emph{Importance of error distribution analysis}

You need to understand the expected uncertainties in your
observations in order to:
\begin{itemize}
    \item predict the amount of observing time you'll need to get
    uncertainties as small as you need them to do your science
    \item answer the question: is scatter in observed data consistent
    with expected uncertainties? If the answer is no, they you know
    you've either learned some astrophysics or you don't understand
    something about your observations. This is especially important in
    astronomy where objects are faint and many projects are pushing
    down into the noise as far as possible. Really we can usually only
    answer this probabilistically. Generally, tests compute the
    probability that the observations are consistent with an expected
    distribution (the null hypothesis). You can then look to see if
    this probability is low, and if so, you can reject the null
    hypothesis.
    \item interpret your results in the context of a scientific
    prediction
\end{itemize}
\emph{Confidence levels}

For example, say we want to know whether some single point
is consistent with expectations, e.g., we see a bright point in
multiple measurements of a star, and want to know whether the star
flared. Say we have a time sequence with known mean and variance, and
we obtain a new point, and want to know whether it is consistent with
known distribution?

If the form of the probability distribution is known, then
you can calculate the probability of getting a measurement more than
some observed distance from the mean. In the case where the observed
distribution is Gaussian (or approximately so), this is done using the
\emph{error function} (sometimes called $erf(x)$), which is the integral of a
gaussian from some starting value.

Some simple guidelines to keep in mind follow (the actual
situation often requires more sophisticated statistical tests). First,
for Gaussian distributions, you can calculate that 68\% of the points
should fall within $\pm 1\sigma$ from the mean, and 95.3\%
should fall within $\pm 2\sigma$ from the mean. Thus, if you have a
time line of photon fluxes for a star, with $N$ observed points, and a
photon noise $\sigma$ on each measurement, you can test whether the
number of points deviating more than 2$\sigma$ from the mean is much
larger than expected. To decide whether any single point is really
significantly different, you might want to use more stringent
criterion, e.g., 5$\sigma$ rather than a 2$\sigma$ criterion;
a 5$\sigma$ has much higher level of significance. On the other hand, if
you have far more points in the range $2-4\sigma$ brighter or
fainter than you would expect, you may also have a significant
detection of intensity variations (provided you really understand your
uncertainties on the measurements).

Also, note that your observed distribution should be
consistent with your uncertainty estimates given the above guidelines.
If you have a whole set of points, that all fall within 1$\sigma$ of
each other, something is wrong with your uncertainty estimates (or
perhaps your measurements are correlated with each other).

For a series of measurements, one can calculate the
$\chi^{2}$ statistic, and determine how probable this value is,
given the number of points.
    $$ \chi^2 = \sum [(observed(i)-model(i))^2/\sigma_i^2]  $$
A quick estimate of the consistency of the model with the observed
data points can be made using reduced $\chi^{2}$, defined as
$\chi^{2}$ divided by the \emph{degrees of freedom} (number of data points
minus number of parameters).

\textcolor{myBlue}{
    $$  \chi^2 = \sum_N \frac{(y_i-y(x_i))^2}{\sigma_i^2} $$
for $N$ data points. If every point was 1$\sigma$ off,
$\chi^2 \sim$ number of data points. Can calculate the probability
distribution of $\chi^2$. If $\chi^2$ = 98, what's the confidence
interval? Will only get 98 one in a million times? Not a good model.
Is the data consistent with the model?
}

\textcolor{magenta}{Wednesday, February 3}
\subsubsection*{Noise equation: how do we predict expected
uncertainties?}
\emph{Signal-to-noise}

\textcolor{myBlue}{
Shape parameterized by stddev which = $\sqrt{\mu}$
$\rightarrow$ Fundamental! e.g. source emitting 1000 photons has
uncertainty of 100, etc. (telescope, brightness of source, etc. don't
matter$\ldots$ only number of photons, no matter how you got them).
    $$ \frac{S}{N} = \frac{S}{\sqrt{S}} = \sqrt{S} $$
How does $S/N$ change for a given object?
}

Astronomers often describe uncertainties in terms of the fractional
error, e.g. the amplitude of the uncertainty divided by the amplitude
of the quantity being measured (the signal);
often, the inverse of this, referred
to as the signal-to-noise ratio, is used. Given an estimate the number
of photons expected from an object in an observation, we can calulate
the signal-to-noise ratio:
    $$ \frac{S}{N} = \frac{S}{\sqrt{\sigma^2}} $$
which is the inverse of the predicted fractional error ($N/S$).

Consider an object with observed photon flux, $S^{\prime}$
[cm$^{-2}$ s$^{-1}$],
leading to a signal, $S = S^{\prime}Tt$.
In the simplest case (i.e. perfect
instruments), the only noise
source is Poisson statistics from the source, in which case:
    $$ \sigma^2 = S = S^{\prime}Tt $$
    $$ \frac{S}{N} = \sqrt{S} = \sqrt{S^{\prime}Tt} $$
In other words, $S/N$ increases as the square root of the object
brightness, telescope area, efficiency, or exposure time. Note that $S$
is directly observable, so one can calculate $S/N$ for an
observation without knowing the telescope area or exposure time.
We've just broken $S$ down so that you can specifically see the dependence on
telescope area and/or exposure time.

\emph{Background noise}

\textcolor{myBlue}{Rayleigh scattering}

A more realistic case includes the noise contributed from Poisson
statistics of ``background'' light, $B^{\prime}$,
which has units of flux per area
on the \emph{sky}, not the detector
(i.e. a surface brightness); note that this is also usually
given in magnitudes (more on the physical nature of
this later).

    $$ B^{\prime} = \int \frac{B_{\lambda}}{\frac{hc}{\lambda}}
       q_{\lambda}\textrm{d}\lambda $$

\textcolor{myBlue}{$B_{\lambda}$ is the sky brightness at $\lambda
\ldots$? And $B^{\prime}$ is the observed flux from the background.
So B vs. B' is the same as S vs. S'. Something about adding
uncertainties in quadrature$\ldots$}

The amount of background in our measurement depends on
how much sky area we observe. Say we just
use an aperture with area, $A$, so the total observed background counts
is
    $$ AB = AB^{\prime}Tt $$

Again, $B^{\prime}Tt$ is the directly observable quantity,
but we split it into the quantities on which it depends to understand
what factors are important in determining $S/N$.

The total number of photons observed, $O$, is a combination of
photons from the source ($S$), and phtons from the background ($AB$):
    $$ O = S + AB = (S^{\prime} + AB^{\prime})Tt $$
The variance of the total observed counts, from Poisson statistics,
is:
    $$ \sigma^2_O = O =  S + AB = (S^{\prime} + AB^{\prime})Tt $$
To get the desired signal from the object only, we will need to
measure separately the total signal and the background signal to
estimate:
    $$ S \equiv S^{\prime}Tt = O-A <B> $$
where $<B>$ is some estimate we have obtained of the background
surface brightness. The noise in the measurement is
    $$ \sigma^2_S \approx \sigma^2_O =
    S + AB = (S^{\prime} + AB^{\prime})Tt $$
where the approximation is accurate if the background is determined to
high accuracy, which one can do if one measures the background over a
large area, thus getting a large number of background counts (with
correspondingly small fractional error in the measurement).

This leads to a common form of the \emph{noise equation}:
    $$ \frac{S}{N} = \frac{S}{\sqrt{S+AB}}  $$
Breaking out the dependence on exposure time and telescope area, this
is:
    $$ \frac{S}{N} = \frac{S^{\prime}}
    {\sqrt{S^{\prime}+AB^{\prime}}}
    \sqrt{T}\sqrt{t}$$

Limiting regimes:
\begin{itemize}
    \item \emph{signal-limited} case: $S^{\prime}\gg B^{\prime}A$,
        the background goes away and we get
         $$ \frac{S}{N} = \sqrt{S} = \sqrt{S^{\prime}tT} $$
    \item \emph{background-limited} case: $B^{\prime}A\gg S^{\prime}$
        and
        $$ \frac{S}{N} = \frac{S}{\sqrt{BA}} =
        \frac{S^{\prime}}{\sqrt{B^{\prime}A}}\sqrt{tT} $$
\end{itemize}

\textcolor{myBlue}{starlight dominates by factor of 1000 for
m$_* = 12.5$ and m$_{BG} = 20$ ($\Delta m = 7.5 
= -2.5\log\frac{b_{BG}}{b_*}
\rightarrow 10^3$)\\\\
\large$\odot$: \normalsize little circle around star to reduce sky
background noise. Depends on how blurry the star is; determined by
seeing.
\begin{itemize}
    \item Dust scattering in the plane of the solar system
    \item Diffraction limit of space telescope $\sim$ 20th of an
    arcsecond. ($\sim$ angular resolution).
\end{itemize}
}

As one goes to fainter objects, the S/N drops, and it drops faster
when you're background limited. This illustrates the importance of
dark-sky sites, and also the importance of good image quality.

Consider two telescopes of collecting area, $T_1$ and $T_2$.
If we observe for the same exposure time on each and want to know how
much fainter we can see with the larger telescope at a given $S/N$, we
find this for a signal-limited case:
$$ S_2 = \frac{T_1}{T_2}S_1 $$
but find \emph{this} for the background-limited case:
$$ S_2 = \sqrt{\frac{T_1}{T_2}}S_1  $$

\emph{Instrumental noise}

\textcolor{myBlue}{Most common:readout noise. Gaussian, not poisson.
$\mu$ = 0, $\sigma$ = readout noise (depends on detector, e.g. 10
electrons ($\sim$ photons create photoelectrons). Get a smear of 10 no
matter how many photons are coming in. Represented by range around
zero with no exposure time. Mean is zero, so can't subtract it out, it
just blurs what you're looking at. B - background per square
arcsecond, A - square arcsecond. Number of pixels over which it's
spread:
\begin{itemize}
    \item sharper image
    \item 10 arcsec all in one pixel
\end{itemize}
Can change magnification of optics, but there is a balance. Might have
two stars five arcsec apart. Maxiumum resolution is not always the
best! readout noise is very important when the background is very low.
$N_{pix}\sigma_{rn}^2$ compared to $BA$. rn more important for
spectroscopy applications than imaging.
}

In addition to the errors from Poisson statistics (statistical noise),
there may be additional terms from instrumental errors. A common
example of this that is applicable for CCD detectors is readout noise,
which is additive \emph{noise} (with zero mean) that comes from the detector
and is independent of signal level. For a detector whose readout noise
is characterized by $\sigma_{rn}$,
$$ \frac{S}{N} = \frac{S}{\sqrt{S+BA_{pix}+\sigma^2_{rn}}} $$
if a measurement is made in a single pixel. If an object is spread
over $N_{pix}$ pixels, then
$$ \boxed{
 \frac{S}{N} = \frac{S}{\sqrt{S+BA+N_{pix}\sigma^2_{rn}}}
 } $$
\textcolor{myBlue}{This is the key equation!}
For large $\sigma_{rn}$, the behavior is the same as the
background-limited case. This makes it clear that if you have readout
noise, image quality (and/or proper optics to keep an object from
covering too many pixels) is important for maximizing $S/N$. It is
also clear that it is critical to have minimum read-noise for low
background applications (e.g., spectroscopy).

There are other possible additional terms in the noise equation,
arising from things like dark current, digitization noise, errors in
sky determination, errors from photometric technique, etc. (we'll
discuss some of these later on), but in most applications, the three
sources discussed so far - signal noise, background noise, and readout
noise - are the dominant noise sources.

Note the applications where one is likely to be signal dominated,
background dominated, and readout noise dominated.

\textcolor{magenta}{Monday, February 8}

\subsubsection*{Error propagation}

So now we know how to estimate uncertainties of observed count rates.
Let's say we want to make some calculations (e.g., calibration, unit
conversion, averaging, conversion to magnitudes, calculation of
colors, etc.) using these observations: we need to be able to estimate
the uncertainties in the calculated quantities that depend on our
measured quantities.

Consider what happens if you have several known quantities with known
error distributions and you combine these into some new quantity: we
wish to know what the error is in the new quantity.
    $$ x = f(u, v, \ldots) $$
The question is, what is $\sigma_x$ if we know 
$\sigma_u$, $\sigma_v$, etc.?

As long as errors are small:
    $$ x_i - <x> \ \sim \ 
        (u_i - <u>)\Bigg(\frac{\delta x}{\delta u}\Bigg)
       + (v_i - <v>)\Bigg( \frac{\delta x}{\delta v}  \Bigg)
       + \ldots  $$
    $$ \sigma_x^2 = lim(N \rightarrow \infty)\frac{1}{N}
       \sum(u_i - <u>)(v_i - <v>)^2 $$
    $$ \sigma_u^2\Bigg(\frac{\delta x}{\delta u}\Bigg)^2
       + \sigma_v^2\Bigg(\frac{\delta x}{\delta v}\Bigg)^2
       + 2\sigma_{uv}^2\frac{\delta x}{\delta u}
         \frac{\delta x}{\delta v} + \ldots $$
The last term is the \emph{covariance}, which relates to whether
errors are \emph{correlated}.
    $$ \sigma_{uv}^2 = lim(N \rightarrow\infty)\frac{1}{N}
       \sum(u_i - <u>)(v_i - <v>)  $$
If errors are uncorrelated, then $\sigma_{uv} = 0$ becaues there's
equal chance of getting opposite signs on $v_i$ for any given $u_i$.
When working out errors, make sure to consider whether there are
correlated errors. If there are, you \emph{may} be able to reformulate
quantities so that they have independent errors: this can be very
useful.

Examples for \emph{uncorrelated} errrors:
\begin{itemize}
    \item addition/subtraction:
        $$ x = u + v $$
        $$ \sigma_x^2 = \sigma_u^2 + \sigma_v^2 $$
        In this case, errors are said to \emph{add in quadrature}.
    \item multiplication/division:
        $$ x = uv $$
        $$ \sigma_x^2 = v^2\sigma_u^2 + u^2\sigma_v^2 $$
    \item logs:
        $$ x = \ln u $$
        $$ \sigma_x^2 = \frac{\sigma_u^2}{u^2} $$
        Note that when dealing with logarithmic quantities, errors in
        the log correspond to \emph{fractional} errors in the raw
        quantity. 
\end{itemize}

\emph{Distribution of resultant errors}

When propagating errors, even though you can calculate the variances
in the new variables, the distribution of errors in the new variables
is not, in general, the same as the distribution of errors in the
original variables, e.g. if errors in individual variables are
normally distributed, errors in output variable is not necessarily.

When two variables are added, however, the output is normally
distributed.

\subsubsection*{Determining sample parameters: averaging measurements}
We've covered errors in single measurements. Next we turn to averaging
measurements. Say we have multiple observations and want the best
estimate of the mean and variance of the population, e.g. multiple
measurements of stellar brightness. Here we'll define the best
estimate of the mean as the value which maximizes the likelihood that
our estimate equals the true parent population mean.

For equal errors, this estimate just gives our normal expression for
the sample mean:
    $$ \bar{x} = \frac{\sum x_i}{N} $$
Using error propagation, the estimate of the error in the sample mean
is given by:
    $$ ? $$
But what if errors on each observation aren't equal, say for example
we have observations made with several different exposure times? Then
the optimal determination of the mean is using a:
    $$ ? $$
and the estimated error in this value is given by: 
    $$ ? $$
where the $\sigma_i$'s are individual weights/errors
(people often talk about the \emph{weight} of an observation, i.e.
$\frac{1}{\sigma_i^2}$: large weight means small error.

 This is a standard result for determining sample means from a set of
 observations with different weights.

However, there can sometimes be a subtlety in applying this formula,
which has to do with the question: how do we go about choosing the
weights/errors, $\sigma_{i}$? We know we can \emph{estimate} $\sigma$
using Poisson statistics for a given count rate, but remember that
this is a sample variance (which may be based on a single
observation) not the true population variance. This can lead to
biases.

Consider observations of a star made on three nights, with
measurements of 40, 50, and 60 photons. It's clear that the mean
observation is 50 photons. However, beware of the being trapped by
your error \emph{estimates}. From each observation alone, you would estimate
errors of $\sqrt{40}$, $\sqrt{50}$, and $\sqrt{60}$. If you
plug these error estimates into a computation of the weighted mean,
you'll get a mean rate of 48.64.

Using the individual estimates of the variances, we'll bias values to
lower rates, since these will have estimated higher $S/N$.

Note that it's pretty obvious from this example that you should just
weight all observations equally. However, note that this certainly
isn't always the right thing to do. For example, consider the
situation in which you have three exposures of different exposure
times. Here you probably want to give the longer exposures higher
weight (at least, if they are signal or background limited). In this
case, you again don't want to use the individual error estimates or
you'll introduce a bias. There is a simple solution here also: just
weight the observations by the exposure time. However, while this
works fine for Poisson errors (variances proportional to count rate),
it isn't strictly correct if there are instrumental errors as well
which don't scale with exposure time. For example, the presence of
readout noise can have this effect; if all exposures are readout
noise dominated, then one would want to weight them equally, if
readout noise dominates the shorter but not the longer exposures,
once would want to weight the longer exposures even higher than
expected for the exposure time ratios! The only way to properly
average measurements in this case is to estimate a sample mean, then
use this value scaled to the appropriate exposure times as the input
for the Poisson errors.

Another subtlety: averaging counts and converting to magnitudes is
not the same as averaging magnitudes.

\emph{Can you split exposures?}

Although from $S/N$ considerations, one can determine the required
number of counts you need (exposure time) to do your science, when
observing, one must also consider the question of whether this time
should be collected in single or in multiple exposures, i.e. how long
individual exposures should be. There are several reasons why one
might imagine that it is nicer to have a sequence of shorter
exposures rather than one single longer exposure (e.g., tracking,
monitoring of photometric conditions, cosmic ray rejection,
saturation issues), so we need to consider whether doing this results
in poorer $S/N$.

Consider the object with photon flux $S'$,
background surface brightness $B'$, and detector
with readout noise $\sigma_{rn}$. A single short exposure of
time $t$ has a variance:
    $$ \sigma_S^2 = S'Tt + B'ATt + N_{pix}\sigma_{rn}^2 $$
If $N$ exposures are summed, the resulting variance will be
    $$ ? $$
If a single long exposure of length Nt is taken, we get 
    $$ ? $$
The ratio of the noises, or the inverse ratio of the S/N (since the
total signal measured is the same in both cases), is
    $$ ? $$
In the signal- or background-limited regimes, exposures can be added
with no loss of S/N. However, if readout noise is significant, then
splitting exposures leads to reduced S/N.

\subsubsection*{Random errors vs systematic errors}
So far, we've been discussing random errors. There is an additional,
usually more troublesome, type of errors known as systematic errors.
These don't occur randomly but rather are correlated with some,
possibly unknown, variable relating to your observations.

EXAMPLE : flat fielding

EXAMPLE : WFPC2 CTE

Note also that in some cases, systematic errors can masquerade as
random errors in your test observations (or be missing altogether if
you don't take data in exactly the same way), but actually be
systematic in your science observations.

EXAMPLE: flat fielding, subpixel QE variations.

Note that error analysis from expected random errors may be the only
clue you get to discovering systematic errors. To discover systematic
errors, plot residuals vs. everything

\end{document}































